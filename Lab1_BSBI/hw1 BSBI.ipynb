{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1 布尔查询之BSBI与索引压缩\n",
    "\n",
    "本次作业使用斯坦福大学[CS 276 / LING 286: Information Retrieval and Web Search](https://web.stanford.edu/class/cs276/)课程的代码框架来实现。具体来说主要包含的内容有：\n",
    "1. [索引构建 (40%)](#索引构建与检索-(40%)) 使用BSBI方法模拟在内存不足的情况下的索引构建方式，并应用于布尔查询\n",
    "2. [索引压缩 (30%)](#索引压缩-(30%)) 使用可变长编码对构建的索引进行压缩\n",
    "3. [布尔检索 (10%)](#布尔联合检索-(10%)) 对空格分隔的单词查询进行联合（与）布尔检索\n",
    "3. [实验报告 (10%)](#Report-(25%)) 描述你的代码并回答一些问题\n",
    "4. [额外的编码方式 (10%)](#额外的编码方式-(10%)) 鼓励使用额外的编码方式对索引进行压缩 (例如, gamma-encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can add additional imports here\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import array\n",
    "import os\n",
    "import timeit\n",
    "import contextlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">WO说：本代码已上传至Github仓库：https://github.com/Hanpiyd/Information-Retrieval.git 。接下来的报告内容将以\n",
    "    Markdown形式呈现，字体为蓝色（区分原有Markdown）</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验使用的文本数据是stanford.edu域下的网页内容，可从http://web.stanford.edu/class/cs276/pa/pa1-data.zip 下载。以下代码将大约170MB的文本数据下载到当前目录下，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "data_url = 'http://web.stanford.edu/class/cs276/pa/pa1-data.zip'\n",
    "data_dir = 'pa1-data'\n",
    "urllib.request.urlretrieve(data_url, data_dir+'.zip')\n",
    "zip_ref = zipfile.ZipFile(data_dir+'.zip', 'r')\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之后构建的索引会被存储到`output_dir`，`tmp`会存储测试数据（toy-data）所生成的一些临时文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    os.mkdir('output_dir')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "try: \n",
    "    os.mkdir('tmp')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "try: \n",
    "    os.mkdir('toy_output_dir')\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在数据目录下有10个子目录（命名0-9）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(os.listdir('pa1-data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每一个子目录下的文件都包含一个独立网页的内容。可以认为在同一子目录下没有同名文件，即每个文件的绝对路径不会相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3dradiology.stanford.edu_',\n",
       " '3dradiology.stanford.edu_patient_care_Case%2520studies_AVM.html',\n",
       " '3dradiology.stanford.edu_patient_care_case_studies.html',\n",
       " '5-sure.stanford.edu_',\n",
       " '50years.stanford.edu_',\n",
       " 'a3cservices.stanford.edu_awards_nominate_',\n",
       " 'a3cservices.stanford.edu_facilities_',\n",
       " 'a3cservices.stanford.edu_lead_',\n",
       " 'aa.stanford.edu_',\n",
       " 'aa.stanford.edu_about_aviation.php']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(os.listdir('pa1-data/0'))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有的网页内容已经经过处理，仅包含由空格分隔开的单词，不再需要进行额外的标准化工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3d radiology lab stanford university school of medicine stanford school of medicine 3d and quantitative imaging in the department of radiology search this site only stanford medical sites ways to give find a person alumni lane library ways to give find a person about us mission to develop and apply innovative techniques for efficient quantitative analysis and display of medical imaging data through interdisciplinary collaboration goals education to train physicians and technologists locally and worldwide in the latest developments in 3d and quantitative imaging research to develop new approaches to the exploration analysis and quantitative assesment of diagnostic images that result in a new and or more cost effective diagnostic approaches and b new techniques for the design and planning and monitoring of therapy patient care to deliver valid clinically relevant visualization and analysis of medical imaging data to the stanford community locations richard m lucas magnetic resonance imaging center 1201 welch rd p170 stanford ca 94305 5488 650 725 8432 james h clark center 318 campus drive s344 stanford ca 94305 5450 650 725 6862 directions to the 3qd lab local hotels you are here stanford medicine school of medicine departments radiology 3dq laboratory navigation for this section 3dq laboratory home education research patient care industry about us site navigation home education overview visiting fellowships reading list research overview research activities research opportunities patient care overview for physicians for patients protocol development 3dq management software case studies testimonials industry overview infrastructure and services about us faculty and staff history of the lab resources and equipment contact information jobs getting to the 3dq lab stanford medicine resources stanford medicine getting care overview find a physician find a clinical care center stanford hospital & clinics lucile packard children's hospital emergency research overview school of medicine news & resources clinical trials departments institutes & centers faculty profiles education & training overview school of medicine programs admissions continuing medical education alumni lane medical library community overview stanford health library community newsletter volunteering public service & community partnerships renewal & new building projects about us overview news careers ways to give find a person contact us maps & directions the dean's newsletter stanford university footer links contact us directions members only 2009 stanford school of medicine terms of use powered by irt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('pa1-data/0/3dradiology.stanford.edu_', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作业目录下有一个小的数据集文件夹`toy-data`。在使用完整网页数据集之前，我们会用它来测试我们的代码是否正确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_dir = 'toy-data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 索引构建与检索 (40%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作业的第一部分是使用**blocked sort-based indexing (BSBI)** 算法来构建倒排索引并实现布尔检索。关于BSBI算法可以参考老师课件或者斯坦福教材[Section 4.2](http://nlp.stanford.edu/IR-book/pdf/04const.pdf)。以下摘自教材内容\n",
    "\n",
    "> To construct an index, we first make a pass through the collection assembling all term-docID pairs. We then sort the pairs with the term as the dominant key and docID as the secondary key. Finally, we organize the docIDs for each term into a postings list and compute statistics like term and document frequency. For small collections, all this can be done in memory. \n",
    "\n",
    "对于无法在内存一次性处理的较大数据集，将会使用到二级存储（如：磁盘）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IdMap\n",
    "\n",
    "再次引用教材 Section 4.2:\n",
    "\n",
    "> To make index construction more efficient, we represent terms as termIDs (instead of strings), where each termID is a unique serial number. We can build the mapping from terms to termIDs on the fly while we are processing the collection. Similarly, we also represent documents as docIDs (instead of strings).\n",
    "\n",
    "我们首先定义一个辅助类`IdMap`，用于将字符串和数字ID进行相互映射，以满足我们在term和termID、doc和docID间转换的需求。\n",
    "\n",
    "实现以下代码中的`_get_str` 和 `_get_id`函数，IdMap类的唯一接口是`__getitem__`，它是一个特殊函数，重写了下标运算`[]`,根据下标运算键的类型得到正确的映射值（如果不存在需要添加）。（特殊函数可参考[官方文档](https://docs.python.org/3.7/reference/datamodel.html#special-method-names)）\n",
    "<br>\n",
    "<br>\n",
    "我们会用到字典来将字符串转换为数字，用列表来将数字转换为字符串。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">对于IdMap部分需要完成的两个函数-_get_str和_get_id，处理略微不同。虽然_get_str在前，但是这里选择先完成更重要的_get_id，\n",
    "先判断要查询的字符串是否是字典中的Key，如果不是则将（该字符串，字典长度）这个Key-Value对存入字典（字典长度永远不可能重复，因此很适合作为id），\n",
    "同时将该字符串存入列表中（由于字典和列表的长度一定相同，因此当id作为下标时从列表中取出的一定是该字符串）；如果是，则直接利用字典的特性得到相应\n",
    "的Value-id即可。而对于_get_str，直接根据前面的叙述使用id作为下标从列表取相应元素即可。</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdMap:\n",
    "    \"\"\"Helper class to store a mapping from strings to ids.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.str_to_id = {}\n",
    "        self.id_to_str = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return number of terms stored in the IdMap\"\"\"\n",
    "        return len(self.id_to_str)\n",
    "        \n",
    "    def _get_str(self, i):\n",
    "        \"\"\"Returns the string corresponding to a given id (`i`).\"\"\"\n",
    "        ### Begin your code\n",
    "        return self.id_to_str[i]\n",
    "        ### End your code\n",
    "        \n",
    "    def _get_id(self, s):\n",
    "        \"\"\"Returns the id corresponding to a string (`s`). \n",
    "        If `s` is not in the IdMap yet, then assigns a new id and returns the new id.\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        if s not in self.str_to_id.keys():\n",
    "            self.str_to_id[s] = len(self.str_to_id)\n",
    "            self.id_to_str.append(s)\n",
    "        return self.str_to_id[s]\n",
    "        ### End your code\n",
    "            \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"If `key` is a integer, use _get_str; \n",
    "           If `key` is a string, use _get_id;\"\"\"\n",
    "        if type(key) is int:\n",
    "            return self._get_str(key)\n",
    "        elif type(key) is str:\n",
    "            return self._get_id(key)\n",
    "        else:\n",
    "            raise TypeError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "确保代码能通过以下简单测试样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "testIdMap = IdMap()\n",
    "assert testIdMap['a'] == 0, \"Unable to add a new string to the IdMap\"\n",
    "assert testIdMap['bcd'] == 1, \"Unable to add a new string to the IdMap\"\n",
    "assert testIdMap['a'] == 0, \"Unable to retrieve the id of an existing string\"\n",
    "assert testIdMap[1] == 'bcd', \"Unable to retrive the string corresponding to a\\\n",
    "                                given id\"\n",
    "try:\n",
    "    testIdMap[2]\n",
    "except IndexError as e:\n",
    "    assert True, \"Doesn't throw an IndexError for out of range numeric ids\"\n",
    "assert len(testIdMap) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之后会需要你自己来写测试样例来确保你的程序正常运行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将倒排列表编码成字节数组\n",
    "\n",
    "为了高效地从磁盘读写倒排列表（文档ID），我们将其存储为字节数组的形式。代码提供了`UncompressedPostings`类来用静态函数实现对倒排列表的编码和解码。在之后的任务中你需要使用该接口实现索引压缩版本（可变长编码）。\n",
    "\n",
    "参考:\n",
    "1. https://docs.python.org/3/library/array.html\n",
    "2. https://pymotw.com/3/array/#module-array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UncompressedPostings:\n",
    "    \n",
    "    @staticmethod\n",
    "    def encode(postings_list):\n",
    "        \"\"\"Encodes postings_list into a stream of bytes\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        postings_list: List[int]\n",
    "            List of docIDs (postings)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        bytes\n",
    "            bytearray representing integers in the postings_list\n",
    "        \"\"\"\n",
    "        return array.array('L', postings_list).tobytes()\n",
    "        \n",
    "    @staticmethod\n",
    "    def decode(encoded_postings_list):\n",
    "        \"\"\"Decodes postings_list from a stream of bytes\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        encoded_postings_list: bytes\n",
    "            bytearray representing encoded postings list as output by encode \n",
    "            function\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        List[int]\n",
    "            Decoded list of docIDs from encoded_postings_list\n",
    "        \"\"\"\n",
    "        \n",
    "        decoded_postings_list = array.array('L')\n",
    "        decoded_postings_list.frombytes(encoded_postings_list)\n",
    "        return decoded_postings_list.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行以下代码查看其工作方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x03\\x00\\x00\\x00'\n",
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "x = UncompressedPostings.encode([1,2,3])\n",
    "print(x)\n",
    "print(UncompressedPostings.decode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 磁盘上的倒排索引\n",
    "\n",
    "> With main memory insufficient, we need to use an external sorting algorithm, that is, one that uses disk. For acceptable speed, the central requirement of such an algorithm is that it minimize the number of random disk seeks during sorting - sequential disk reads are far faster than seeks. \n",
    "\n",
    "在这一部分我们提供了一个基类`InvertedIndex`，之后会在此基础上构建它的子类`InvertedIndexWriter`, `InvertedIndexIterator` 和 `InvertedIndexMapper`。在Python中我们常用`cPickle`进行序列化，但是它并不支持部分读和部分写，无法满足BSBI算法的需要，所以我们需要定义自己的存储方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndex:\n",
    "    \"\"\"A class that implements efficient reads and writes of an inverted index \n",
    "    to disk\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    postings_dict: Dictionary mapping: termID->(start_position_in_index_file, \n",
    "                                                number_of_postings_in_list,\n",
    "                                               length_in_bytes_of_postings_list)\n",
    "        This is a dictionary that maps from termIDs to a 3-tuple of metadata \n",
    "        that is helpful in reading and writing the postings in the index file \n",
    "        to/from disk. This mapping is supposed to be kept in memory. \n",
    "        start_position_in_index_file is the position (in bytes) of the postings \n",
    "        list in the index file\n",
    "        number_of_postings_in_list is the number of postings (docIDs) in the \n",
    "        postings list\n",
    "        length_in_bytes_of_postings_list is the length of the byte \n",
    "        encoding of the postings list\n",
    "    \n",
    "    terms: List[int]\n",
    "        A list of termIDs to remember the order in which terms and their \n",
    "        postings lists were added to index. \n",
    "        \n",
    "        After Python 3.7 we technically no longer need it because a Python dict \n",
    "        is an OrderedDict, but since it is a relatively new feature, we still\n",
    "        maintain backward compatibility with a list to keep track of order of \n",
    "        insertion. \n",
    "    \"\"\"\n",
    "    def __init__(self, index_name, postings_encoding=None, directory=''):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        index_name (str): Name used to store files related to the index\n",
    "        postings_encoding: A class implementing static methods for encoding and \n",
    "            decoding lists of integers. Default is None, which gets replaced\n",
    "            with UncompressedPostings\n",
    "        directory (str): Directory where the index files will be stored\n",
    "        \"\"\"\n",
    "\n",
    "        self.index_file_path = os.path.join(directory, index_name+'.index')\n",
    "        self.metadata_file_path = os.path.join(directory, index_name+'.dict')\n",
    "\n",
    "        if postings_encoding is None:\n",
    "            self.postings_encoding = UncompressedPostings\n",
    "        else:\n",
    "            self.postings_encoding = postings_encoding\n",
    "        self.directory = directory\n",
    "\n",
    "        self.postings_dict = {}\n",
    "        self.terms = []         #Need to keep track of the order in which the \n",
    "                                #terms were inserted. Would be unnecessary \n",
    "                                #from Python 3.7 onwards\n",
    "\n",
    "    def __enter__(self):\n",
    "        \"\"\"Opens the index_file and loads metadata upon entering the context\"\"\"\n",
    "        # Open the index file\n",
    "        self.index_file = open(self.index_file_path, 'rb+')\n",
    "\n",
    "        # Load the postings dict and terms from the metadata file\n",
    "        with open(self.metadata_file_path, 'rb') as f:\n",
    "            self.postings_dict, self.terms = pkl.load(f)\n",
    "            self.term_iter = self.terms.__iter__()                       \n",
    "\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exception_type, exception_value, traceback):\n",
    "        \"\"\"Closes the index_file and saves metadata upon exiting the context\"\"\"\n",
    "        # Close the index file\n",
    "        self.index_file.close()\n",
    "        \n",
    "        # Write the postings dict and terms to the metadata file\n",
    "        with open(self.metadata_file_path, 'wb') as f:\n",
    "            pkl.dump([self.postings_dict, self.terms], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为是在与磁盘上的文件进行交互，所以我们提供了`__enter__`和`__exit__`函数，它使得我们能够像使用python中文件IO一样使用`with`语句。（参考[上下文管理器官方文档](https://docs.python.org/3/library/contextlib.html)）\n",
    "\n",
    "以下是使用 `InvertedIndexWriter` 上下文管理器的实例:\n",
    "\n",
    "```python\n",
    "with InvertedIndexWriter('test', directory='tmp/') as index:\n",
    "    # Some code here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 索引\n",
    "\n",
    "> BSBI (i) segments the collection into parts of equal size, (ii) sorts the termID-docID pairs of each part in memory, (iii) stores intermediate sorted results on disk, and (iv) merges all intermediate results into the final index. \n",
    "\n",
    "你需要将每一个子目录当做一个块（block），并且在构建索引的过程中每次只能加载一个块（模拟内存不足）。注意到我们是将操作系统意义上的块进行了抽象。你可以认为每个块足够小，能被装载进内存。\n",
    "\n",
    "在这一部分，我们将阶段性地构造类`BSBIIndex`。函数`index`给出了BSBI算法的框架，而你的工作则是在接下来的部分中实现函数`parse_block`, `invert_write` 和 `merge`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not make any changes here, they will be overwritten while grading\n",
    "class BSBIIndex:\n",
    "    \"\"\" \n",
    "    Attributes\n",
    "    ----------\n",
    "    term_id_map(IdMap): For mapping terms to termIDs\n",
    "    doc_id_map(IdMap): For mapping relative paths of documents (eg \n",
    "        0/3dradiology.stanford.edu_) to docIDs\n",
    "    data_dir(str): Path to data\n",
    "    output_dir(str): Path to output index files\n",
    "    index_name(str): Name assigned to index\n",
    "    postings_encoding: Encoding used for storing the postings.\n",
    "        The default (None) implies UncompressedPostings\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, output_dir, index_name = \"BSBI\", \n",
    "                 postings_encoding = None):\n",
    "        self.term_id_map = IdMap()\n",
    "        self.doc_id_map = IdMap()\n",
    "        self.data_dir = data_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.index_name = index_name\n",
    "        self.postings_encoding = postings_encoding\n",
    "\n",
    "        # Stores names of intermediate indices\n",
    "        self.intermediate_indices = []\n",
    "        \n",
    "    def save(self):\n",
    "        \"\"\"Dumps doc_id_map and term_id_map into output directory\"\"\"\n",
    "        \n",
    "        with open(os.path.join(self.output_dir, 'terms.dict'), 'wb') as f:\n",
    "            pkl.dump(self.term_id_map, f)\n",
    "        with open(os.path.join(self.output_dir, 'docs.dict'), 'wb') as f:\n",
    "            pkl.dump(self.doc_id_map, f)\n",
    "    \n",
    "    def load(self):\n",
    "        \"\"\"Loads doc_id_map and term_id_map from output directory\"\"\"\n",
    "        \n",
    "        with open(os.path.join(self.output_dir, 'terms.dict'), 'rb') as f:\n",
    "            self.term_id_map = pkl.load(f)\n",
    "        with open(os.path.join(self.output_dir, 'docs.dict'), 'rb') as f:\n",
    "            self.doc_id_map = pkl.load(f)\n",
    "            \n",
    "    def index(self):\n",
    "        \"\"\"Base indexing code\n",
    "        \n",
    "        This function loops through the data directories, \n",
    "        calls parse_block to parse the documents\n",
    "        calls invert_write, which inverts each block and writes to a new index\n",
    "        then saves the id maps and calls merge on the intermediate indices\n",
    "        \"\"\"\n",
    "        for block_dir_relative in sorted(next(os.walk(self.data_dir))[1]):\n",
    "            td_pairs = self.parse_block(block_dir_relative)\n",
    "            index_id = 'index_'+block_dir_relative\n",
    "            self.intermediate_indices.append(index_id)\n",
    "            with InvertedIndexWriter(index_id, directory=self.output_dir, \n",
    "                                     postings_encoding=\n",
    "                                     self.postings_encoding) as index:\n",
    "                self.invert_write(td_pairs, index)\n",
    "                td_pairs = None\n",
    "        self.save()\n",
    "        with InvertedIndexWriter(self.index_name, directory=self.output_dir, \n",
    "                                 postings_encoding=\n",
    "                                 self.postings_encoding) as merged_index:\n",
    "            with contextlib.ExitStack() as stack:\n",
    "                indices = [stack.enter_context(\n",
    "                    InvertedIndexIterator(index_id, \n",
    "                                          directory=self.output_dir, \n",
    "                                          postings_encoding=\n",
    "                                          self.postings_encoding)) \n",
    "                 for index_id in self.intermediate_indices]\n",
    "                self.merge(indices, merged_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解析\n",
    "\n",
    "> The function `parse_block`  parses documents into termID-docID pairs and accumulates the pairs in memory until a block of a fixed size is full. We choose the block size to fit comfortably into memory to permit a fast in-memory sort. \n",
    "\n",
    "你需要将每一个子目录当做一个块，`parse_block`接收子目录路径作为参数。同一子目录下所有文件名都是不同的。\n",
    "\n",
    "_注意 - 我们使用 `BSBIIndex` 继承 `BSBIIndex`, 这只是对一个已存在类添加新内容的简单方法。在这里只是用来切分类的定义（jupyter notebook内教学使用，无特殊含义）。_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">在本部分，我们要完成parse_block这一方法，它的功能主要是将文件中的单词流转换为（term_id,doc_id），再将得到的所有元组存入\n",
    "列表作为返回值，首先，我们先初始化存放元组的列表，再将基地址和相对地址进行拼接得到绝对地址。而绝对地址下又有很多子文件，因此通过os.listdir得到\n",
    "所有子文件的名字。接着，以子文件为单位，先拼接相对地址和子文件名打开文件进行数据读取，再利用之前实现的方法得到相应文件的doc_id，同时将读取到\n",
    "的单词流先去掉空格再进行划分。对于得到的每个单词，先利用前面的方法取到term_id,再结合doc_id得到目标元组，接着将元组存入列表即可。最后，结束\n",
    "两层循环后返回列表即可。</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSBIIndex(BSBIIndex):            \n",
    "    def parse_block(self, block_dir_relative):\n",
    "        \"\"\"Parses a tokenized text file into termID-docID pairs\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        block_dir_relative : str\n",
    "            Relative Path to the directory that contains the files for the block\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        List[Tuple[Int, Int]]\n",
    "            Returns all the td_pairs extracted from the block\n",
    "        \n",
    "        Should use self.term_id_map and self.doc_id_map to get termIDs and docIDs.\n",
    "        These persist across calls to parse_block\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        result_list = []\n",
    "        root_addr = os.path.join(self.data_dir, block_dir_relative)\n",
    "        addr_list = os.listdir(root_addr)\n",
    "        for addr in sorted(addr_list):\n",
    "            with open(os.path.join(root_addr, addr), 'r') as f:\n",
    "                doc_id = self.doc_id_map._get_id(os.path.join(block_dir_relative, addr))\n",
    "                word_list = f.read().strip().split()\n",
    "                for word in word_list:\n",
    "                    term_id = self.term_id_map._get_id(word)\n",
    "                    term_doc_pair = [term_id, doc_id]\n",
    "                    result_list.append(term_doc_pair)\n",
    "        return result_list\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观察函数在测试数据上是否正常运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i'm fine , thank you\n",
      "\n",
      "hi hi\n",
      "how are you ?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('toy-data/0/fine.txt', 'r') as f:\n",
    "    print(f.read())\n",
    "with open('toy-data/0/hello.txt', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0],\n",
       " [1, 0],\n",
       " [2, 0],\n",
       " [3, 0],\n",
       " [4, 0],\n",
       " [5, 1],\n",
       " [5, 1],\n",
       " [6, 1],\n",
       " [7, 1],\n",
       " [4, 1],\n",
       " [8, 1]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BSBI_instance = BSBIIndex(data_dir=toy_dir, output_dir = 'tmp/', index_name = 'toy')\n",
    "BSBI_instance.parse_block('0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "写一些测试样例来确保`parse_block`方法正常运行（如：相同单词出现时是相同ID）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin your code\n",
    "\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 倒排表\n",
    "\n",
    "> The block is then inverted and written to disk. Inversion involves two steps. First, we sort the termID-docID pairs. Next, we collect all termID-docID pairs with the same termID into a postings list, where a posting is simply a docID. The result, an inverted index for the block we have just read, is then written to disk.\n",
    "\n",
    "在这一部分我们添加函数`invert_write`来实现由termID-docID对构建倒排表。 \n",
    "\n",
    "但是，我们首先需要实现类`InvertedIndexWriter`。和列表类似，该类提供了append函数，但是倒排表不会存储在内存中而是直接写入到磁盘里。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">在本部分我们要完成append方法，该方法具体要完成的三步内容已经以注释的形式写在了方法中，我们只需要对其进行完成即可。\n",
    "首先使用前面提供的默认方法对参数postings_list进行编码。接着打开索引文件，先将文件指针移动到文件的尾部，用tell方法取到后即为开始位置，剩余的\n",
    "number_of_postings_in_list和length_in_bytes_of_postings_list直接利用len方法即可完成，将得到的三个数组进行组合就得到了作为Value的三元组。\n",
    "再将term和term到三元组的Key-Value对分别存放于列表和字典，再将编码后的postings_list写入索引文件即可。</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndexWriter(InvertedIndex):\n",
    "    \"\"\"\"\"\"\n",
    "    def __enter__(self):\n",
    "        self.index_file = open(self.index_file_path, 'wb+')              \n",
    "        return self\n",
    "\n",
    "    def append(self, term, postings_list):\n",
    "        \"\"\"Appends the term and postings_list to end of the index file.\n",
    "        \n",
    "        This function does three things, \n",
    "        1. Encodes the postings_list using self.postings_encoding\n",
    "        2. Stores metadata in the form of self.terms and self.postings_dict\n",
    "           Note that self.postings_dict maps termID to a 3 tuple of \n",
    "           (start_position_in_index_file, \n",
    "           number_of_postings_in_list, \n",
    "           length_in_bytes_of_postings_list)\n",
    "        3. Appends the bytestream to the index file on disk\n",
    "\n",
    "        Hint: You might find it helpful to read the Python I/O docs\n",
    "        (https://docs.python.org/3/tutorial/inputoutput.html) for\n",
    "        information about appending to the end of a file.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        term:\n",
    "            term or termID is the unique identifier for the term\n",
    "        postings_list: List[Int]\n",
    "            List of docIDs where the term appears\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        postings_list_encoded = self.postings_encoding.encode(postings_list)\n",
    "        with open(self.index_file_path,'r+b') as f:\n",
    "            f.seek(0,2)\n",
    "            start_position = f.tell()\n",
    "            number_of_posting = len(postings_list)\n",
    "            length_in_bytes = len(postings_list_encoded)\n",
    "            self.terms.append(term)\n",
    "            self.postings_dict[term] = (start_position, number_of_posting, length_in_bytes)\n",
    "            f.write(postings_list_encoded)\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尽管还没有实现读取索引的类，我们还是可以用以下测试代码检测我们的实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with InvertedIndexWriter('test', directory='tmp/') as index:\n",
    "    index.append(1, [2, 3, 4])\n",
    "    index.append(2, [3, 4, 5])\n",
    "    index.index_file.seek(0)\n",
    "    assert index.terms == [1,2], \"terms sequence incorrect\"\n",
    "    assert index.postings_dict == {1: (0, 3, len(UncompressedPostings.encode([2,3,4]))), \n",
    "                                   2: (len(UncompressedPostings.encode([2,3,4])), 3, \n",
    "                                       len(UncompressedPostings.encode([3,4,5])))}, \"postings_dict incorrect\"\n",
    "    assert UncompressedPostings.decode(index.index_file.read()) == [2, 3, 4, 3, 4, 5], \"postings on disk incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们实现 `invert_write`，它将解析得到的td_pairs转换成倒排表，并使用`InvertedIndexWriter` 类将其写入磁盘。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">我们接下来实现invert_write方法，就是将我们前面得到的存放元组的列表转换为倒排表并用append方法写入磁盘。首先根据我们的\n",
    "要求先初始化一个字典，以Key:term-Value:Postings_list的方法构造。接着遍历参数列表，如果元组相应的term还没有在字典出来，则存入一个term-空列表的对，\n",
    "如果已经存在，则只需要从将doc_id取出放入相应的列表即可。接着将字典按照term_id大小进行排序，再通过循环将term_id和排好序的相应postings_list借助\n",
    "InvertedIndexWriter的append方法写入索引文件即可。</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSBIIndex(BSBIIndex):\n",
    "    def invert_write(self, td_pairs, index):\n",
    "        \"\"\"Inverts td_pairs into postings_lists and writes them to the given index\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        td_pairs: List[Tuple[Int, Int]]\n",
    "            List of termID-docID pairs\n",
    "        index: InvertedIndexWriter\n",
    "            Inverted index on disk corresponding to the block       \n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        result_dict = {}\n",
    "        for t,d in td_pairs:\n",
    "            if result_dict.get(t) is None:\n",
    "                result_dict[t] = []\n",
    "            result_dict[t].append(d)\n",
    "        seq_key = sorted(result_dict.keys())\n",
    "        for i in seq_key:\n",
    "            result_dict[i] = sorted(result_dict[i])\n",
    "            index.append(i, result_dict[i])\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以在测试数据上读取一个块并观察倒排索引中包含的内容。\n",
    "仿照`InvertedIndexWriter`部分写一些测试样例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin your code\n",
    "\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 合并\n",
    "> The algorithm simultaneously merges the ten blocks into one large merged index. To do the merging, we open all block files simultaneously, and maintain small read buffers for the ten blocks we are reading and a write buffer for the final merged index we are writing. \n",
    "\n",
    "Python中的迭代模型非常自然地符合我们维护一个小的读缓存的要求。我们可以迭代地从磁盘上每次读取文件的一个倒排列表。我们通过构建`InvertedIndex`的子类`InvertedIndexIterator`来完成这个迭代任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">在本部分，主要完成_initialization_hook和__next__两个方法，前者只需要初始化一个指向开始位置（即赋值为零）的变量即可，\n",
    "而后者则稍微有些复杂。对于__next__方法，我们要先判断当前指针是否已经越界，如果越界则直接抛出迭代，如果不这么写后面在merge的时候会出现错误（亲身\n",
    "经历），如果不越界则继续接下来的操作：先利用指针取出相应的term，再利用term从字典中取出对应的三元组，接着利用开始位置和字节长度将postings_list从\n",
    "索引文件中读出并解码，接着将指针向后移一位方便后续的处理。最后将取到的term和postings_list打包返回即可。</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndexIterator(InvertedIndex):\n",
    "    \"\"\"\"\"\"\n",
    "    def __enter__(self):\n",
    "        \"\"\"Adds an initialization_hook to the __enter__ function of super class\n",
    "        \"\"\"\n",
    "        super().__enter__()\n",
    "        self._initialization_hook()\n",
    "        return self\n",
    "\n",
    "    def _initialization_hook(self):\n",
    "        \"\"\"Use this function to initialize the iterator\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        self.curr_position_ptr = 0\n",
    "        ### End your code\n",
    "\n",
    "    def __iter__(self): \n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        \"\"\"Returns the next (term, postings_list) pair in the index.\n",
    "        \n",
    "        Note: This function should only read a small amount of data from the \n",
    "        index file. In particular, you should not try to maintain the full \n",
    "        index file in memory.\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        if self.curr_position_ptr < len(self.terms):\n",
    "            term = self.terms[self.curr_position_ptr]\n",
    "            start_potision, number_of_posting, length_of_bytes = self.postings_dict[term]\n",
    "            self.index_file.seek(start_potision)\n",
    "            postings_list_encoded = self.index_file.read(length_of_bytes)\n",
    "            postings_list = self.postings_encoding.decode(postings_list_encoded)\n",
    "            self.curr_position_ptr += 1 \n",
    "            return term, postings_list\n",
    "        else:\n",
    "            raise StopIteration\n",
    "        ### End your code\n",
    "\n",
    "    def delete_from_disk(self):\n",
    "        \"\"\"Marks the index for deletion upon exit. Useful for temporary indices\n",
    "        \"\"\"\n",
    "        self.delete_upon_exit = True\n",
    "\n",
    "    def __exit__(self, exception_type, exception_value, traceback):\n",
    "        \"\"\"Delete the index file upon exiting the context along with the\n",
    "        functions of the super class __exit__ function\"\"\"\n",
    "        self.index_file.close()\n",
    "        if hasattr(self, 'delete_upon_exit') and self.delete_upon_exit:\n",
    "            os.remove(self.index_file_path)\n",
    "            os.remove(self.metadata_file_path)\n",
    "        else:\n",
    "            with open(self.metadata_file_path, 'wb') as f:\n",
    "                pkl.dump([self.postings_dict, self.terms], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了测试以上代码，我们先用`InvertedIndexWriter` 创建索引，然后再迭代遍历。写一些小的测试样例观察它是否正常运行。至少你得写一个测试，手工构建一个小的索引，用`InvertedIndexWriter`将他们写入磁盘，然后用`InvertedIndexIterator`迭代遍历倒排索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin your code\n",
    "\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> During merging, in each iteration, we select the lowest termID that has not been processed yet using a priority queue or a similar data structure. All postings lists for this termID are read and merged, and the merged list is written back to disk. Each read buffer is refilled from its file when necessary.\n",
    "\n",
    "我们将使用`InvertedIndexIterator`来完成读取的部分，用`InvertedIndexWriter`来将合并后的索引写入磁盘。\n",
    "\n",
    "注意到我们之前一直使用`with` 语句来打开倒排索引文件，但是现在需要程序化地完成这项工作。可以看到我们在基类`BSBIIndex`的`index`函数中使用了`contextlib`([参考文档](https://docs.python.org/3/library/contextlib.html#contextlib.ExitStack))\n",
    "你的任务是合并*打开的*`InvertedIndexIterator`对象（倒排列表），并且通过一个`InvertedIndexWriter`对象每次写入一个文档列表。\n",
    "\n",
    "既然我们知道文档列表已经排过序了，那么我们可以在线性时间内对它们进行合并排序。事实上`heapq`([参考文档](https://docs.python.org/3.0/library/heapq.html)) 是Python中一个实现了堆排序算法的标准模板。另外它还包含一个实用的函数`heapq.merge`，可以将多个已排好序的输入（可迭代）合并成一个有序的输出并返回它的迭代器。它不仅可以用来合并倒排列表，也可以合并倒排索引词典。\n",
    "\n",
    "为了让你快速上手`heapq.merge`函数，我们提供了一个简单的使用样例。给出两个以动物和树平均寿命排序的列表，我们希望合并它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Giraffe', 28)\n",
      "('Rhinoceros', 40)\n",
      "('Gray Birch', 50)\n",
      "('Indian Elephant', 70)\n",
      "('Black Willow', 70)\n",
      "('Golden Eagle', 80)\n",
      "('Basswood', 100)\n",
      "('Box turtle', 123)\n",
      "('Bald Cypress', 600)\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "animal_lifespans = [('Giraffe', 28), \n",
    "                   ('Rhinoceros', 40), \n",
    "                   ('Indian Elephant', 70), \n",
    "                   ('Golden Eagle', 80), \n",
    "                   ('Box turtle', 123)]\n",
    "\n",
    "tree_lifespans = [('Gray Birch', 50), \n",
    "                  ('Black Willow', 70), \n",
    "                  ('Basswood', 100),\n",
    "                  ('Bald Cypress', 600)]\n",
    "\n",
    "lifespan_lists = [animal_lifespans, tree_lifespans]\n",
    "\n",
    "for merged_item in heapq.merge(*lifespan_lists, key=lambda x: x[1]):\n",
    "    print(merged_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意使用`*`将`lifespan_lists`解包作为参数，并且使用`lambda`函数来给出进行排序的key。如果你对它们不熟悉可以参考文档([unpacking lists](https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists), [lambda expressions](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions))。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">对于merge方法的实现，我们先准备一个temp_term和temp_list，主要是我们之后的循环只能访问当前的term和postings_list，因此\n",
    "为了判断相邻的两个term是否相等，即是否需要合并，我们需要记录之前的term和postings_list（方便term相同时合并）。利用前面介绍的heapq.merge对indices\n",
    "进行处理，此时所有（term,postings_list）元组都按term从小到大排序。接着开始遍历，第一个元素是特殊情况，因为它前面没有元素，那它一定与前面不同，\n",
    "此时我们只需要把它的term和postings_list记录下来即可。如果当前的term和前一个term不等还非第一个元素，说明此时的term和前面的一个（或一批）term已经\n",
    "不同，这时将前面记录的term和postings_list用append方法写入索引文件，然后记录当前的term和postings_list作为新的起点。如果term和前一个的term相同\n",
    "时，记录的term不需要变，而记录的postings_list和当前的postings_list先进行拼接，再利用set的特性进行去重并排序，最后再转换回列表，这里就涉及到\n",
    "了记录list的第二个特性-记录当前一个（或一批）相同term的合并后的postings_list（局部结果）。循环结束后将当前记录的term和postings_list再写入索引\n",
    "文件即可结束（很容易发现无论何种情况最后一个（或一批）记录的term和对应的list都不会在循环被写入，因此我们需要单独处理）。</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "class BSBIIndex(BSBIIndex):\n",
    "    def merge(self, indices, merged_index):\n",
    "        \"\"\"Merges multiple inverted indices into a single index\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        indices: List[InvertedIndexIterator]\n",
    "            A list of InvertedIndexIterator objects, each representing an\n",
    "            iterable inverted index for a block\n",
    "        merged_index: InvertedIndexWriter\n",
    "            An instance of InvertedIndexWriter object into which each merged \n",
    "            postings list is written out one at a time\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        temp_term = None\n",
    "        temp_list = None\n",
    "        for term, postings_list in heapq.merge(*indices):\n",
    "            if term !=temp_term:\n",
    "                if temp_term is None:\n",
    "                    temp_term = term\n",
    "                    temp_list = postings_list\n",
    "                else:\n",
    "                    merged_index.append(temp_term, list(sorted(set(temp_list))) )\n",
    "                    temp_term = term\n",
    "                    temp_list = postings_list\n",
    "            else:\n",
    "                temp_list = temp_list + postings_list\n",
    "        if temp_term is not None:\n",
    "            merged_index.append(temp_term, list(sorted(set(temp_list))))\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先确保它在测试数据上正常运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSBI_instance = BSBIIndex(data_dir=toy_dir, output_dir = 'toy_output_dir', )\n",
    "BSBI_instance.index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来对整个数据集构建索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSBI_instance = BSBIIndex(data_dir='pa1-data', output_dir = 'output_dir', )\n",
    "BSBI_instance.index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你在合并阶段出现了错误，可以利用以下代码来debug。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSBI_instance = BSBIIndex(data_dir='pa1-data', output_dir = 'output_dir', )\n",
    "BSBI_instance.intermediate_indices = ['index_'+str(i) for i in range(10)]\n",
    "with InvertedIndexWriter(BSBI_instance.index_name, directory=BSBI_instance.output_dir, postings_encoding=BSBI_instance.postings_encoding) as merged_index:\n",
    "    with contextlib.ExitStack() as stack:\n",
    "        indices = [stack.enter_context(InvertedIndexIterator(index_id, directory=BSBI_instance.output_dir, postings_encoding=BSBI_instance.postings_encoding)) for index_id in BSBI_instance.intermediate_indices]\n",
    "        BSBI_instance.merge(indices, merged_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 布尔联合检索 (10%)\n",
    "\n",
    "我们将实现BSBIIndex中的`retrieve`函数，对于给定的包含由空格分隔tokens的字符串查询，返回包含查询中所有tokens的文档列表。但是我们并不能迭代遍历整个索引或者将整个索引加载到内存来寻找相应的terms（索引太大）。\n",
    "\n",
    "首先我们要实现`InvertedIndex`的子类`InvertedIndexMapper`，它能够找到对应terms在索引文件中位置并取出它的倒排记录表。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: blue\">_get_postings_list方法的实现其实非常简单，还是得以与前面的工作。使用term从postings_dict我们可以取到对应的三元组，\n",
    "先利用start_position将索引文件的指针移动到相应postings_list的开头，再读取出长为lengths_of_bytes的字节流，读取出postings_list后解码返回即可。\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndexMapper(InvertedIndex):\n",
    "    def __getitem__(self, key):\n",
    "        return self._get_postings_list(key)\n",
    "    \n",
    "    def _get_postings_list(self, term):\n",
    "        \"\"\"Gets a postings list (of docIds) for `term`.\n",
    "        \n",
    "        This function should not iterate through the index file.\n",
    "        I.e., it should only have to read the bytes from the index file\n",
    "        corresponding to the postings list for the requested term.\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        start_position, number_of_posting, length_of_bytes = self.postings_dict[term]\n",
    "        self.index_file.seek(start_position)\n",
    "        postings_list_encoded = self.index_file.read(length_of_bytes)\n",
    "        postings_list = self.postings_encoding.decode(postings_list_encoded)\n",
    "        return postings_list\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "写一些测试样例检测`_get_postings_list`的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin your code\n",
    "\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们获得了查询中terms对应的倒排记录表，接着需要求他们的交集。这部分与我们之前作业的merge方法类似，请实现`sorted_intersect`函数，遍历两个有序列表并在线性时间内合并。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: blue\">对于这部分完成两个列表求交集的布尔检索算法，根据课上所学的知识可以非常简单实现。先初始化两个相应的指针和一个存放\n",
    "结果的列表，接着开始循环即可，终止条件即为其中至少一个列表已遍历完毕。对于两个指针所指向的值，如果不相等的话只需要让较小的指针加一即可，当\n",
    "相等的时候就将相等的值存入列表中，两个指针同时加一。最后返回已经存放了结果的列表即可。</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_intersect(list1, list2):\n",
    "    \"\"\"Intersects two (ascending) sorted lists and returns the sorted result\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    list1: List[Comparable]\n",
    "    list2: List[Comparable]\n",
    "        Sorted lists to be intersected\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List[Comparable]\n",
    "        Sorted intersection        \n",
    "    \"\"\"\n",
    "    ### Begin your code\n",
    "    ptr1 = 0\n",
    "    ptr2 = 0\n",
    "    result_list = []\n",
    "    while ptr1 < len(list1) and ptr2 < len(list2):\n",
    "        if list1[ptr1] < list2[ptr2]:\n",
    "            ptr1 += 1\n",
    "        elif list1[ptr1] > list2[ptr2]:\n",
    "            ptr2 += 1\n",
    "        else:\n",
    "            result_list.append(list1[ptr1])\n",
    "            ptr1 += 1\n",
    "            ptr2 += 1\n",
    "    return result_list\n",
    "    ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单的测试样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin your code\n",
    "\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在可以利用`sorted_intersect` 和 `InvertedIndexMapper`来实现`retrieve`函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: blue\">对于retrieve方法，我们就需要综合前面所实现的两个方法来进行辅助，我们先初始化一个InvertedIndexMapper帮助从索引文件\n",
    "获取相应的postings_list，接着初始化一个列表用来存放以doc_id为元素的中间列表。由于查询输入的是字符串，因此我们要先使用split方法将字符串切分为\n",
    "单词，接着遍历单词，如果存在一个单词并不在所有原文章中出现过，则直接返回空列表（注释规定），如果都出现过，我们只需要先将相应的单词映射为\n",
    "term_id，再利用term_id使用mapper读出相应的postings_list，将所有的postings_list进行交集运算（每次都处理两个，之前处理的结果和下一个列表进行\n",
    "交集，比如说第一个和第二个列表交的结果与第三个列表再进行相交运算）最后就得到了相应的结果列表。但此时的列表是以doc_id为元素的，而返回值希望\n",
    "我们直接使用doc名，因此再借助IdMap将doc_id映射为doc名，最后返回以doc名为单位的列表即可。</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%tee submission/retrieve.py\n",
    "class BSBIIndex(BSBIIndex):\n",
    "    def retrieve(self, query):\n",
    "        \"\"\"Retrieves the documents corresponding to the conjunctive query\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        query: str\n",
    "            Space separated list of query tokens\n",
    "            \n",
    "        Result\n",
    "        ------\n",
    "        List[str]\n",
    "            Sorted list of documents which contains each of the query tokens. \n",
    "            Should be empty if no documents are found.\n",
    "        \n",
    "        Should NOT throw errors for terms not in corpus\n",
    "        \"\"\"\n",
    "        if len(self.term_id_map) == 0 or len(self.doc_id_map) == 0:\n",
    "            self.load()\n",
    "\n",
    "        ### Begin your code\n",
    "        with InvertedIndexMapper(self.index_name, directory=self.output_dir, \n",
    "                                 postings_encoding=\n",
    "                                 self.postings_encoding) as mapper:\n",
    "            result = None\n",
    "            term_list = query.split()\n",
    "            for term in term_list:\n",
    "                term_id = self.term_id_map.str_to_id.get(term)\n",
    "                if term_id is None:\n",
    "                    return []\n",
    "                else:\n",
    "                    temp_list = mapper._get_postings_list(term_id)\n",
    "                    if result is None:\n",
    "                        result = temp_list\n",
    "                    else:\n",
    "                        result = sorted_intersect(result, temp_list)\n",
    "            result_str_list = []\n",
    "            for i in result:\n",
    "                temp_str = self.doc_id_map._get_str(i)\n",
    "                result_str_list.append(temp_str)\n",
    "            return result_str_list\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过一个简单的查询来测试索引在真实数据集上是否正常工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1\\\\cs276.stanford.edu_',\n",
       " '1\\\\cs276a.stanford.edu_',\n",
       " '3\\\\infolab.stanford.edu_TR_CS-TN-95-21.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_an-appraisal-of-probabilistic-models-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_bayesian-network-approaches-to-ir-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_boolean-retrieval-2.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_computing-scores-in-a-complete-search-system-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_dictionaries-and-tolerant-retrieval-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_efficient-scoring-and-ranking-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_inexact-top-k-document-retrieval-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_probabilistic-information-retrieval-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_putting-it-all-together-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_references-and-further-reading-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_the-search-user-experience-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_vector-space-scoring-and-query-operator-interaction-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_weighted-zone-scoring-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_wildcard-queries-2.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_newslides.html',\n",
       " '6\\\\www-nlp.stanford.edu_IR-book_essir2011_',\n",
       " '8\\\\www.stanford.edu_class_cs124_']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BSBI_instance = BSBIIndex(data_dir='pa1-data', output_dir = 'output_dir', )\n",
    "BSBI_instance.retrieve('boolean retrieval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你也可以通过读取文件来测试其中的页面是否真的包含了查询的terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs276 information retrieval and web search cs 276 ling 286 information retrieval and web search spring 2011 pandu nayak and prabhakar raghavan lecture 3 units tu th 4 15 5 30 gates b03 tas sonali aggarwal ivan cui valentin spitkovsky and sandeep sripada staff e mail cs276 spr1011 staff lists stanford edu lectures are also available online and on television through scpd sitn course description basic and advanced techniques for text based information systems efficient text indexing boolean and vector space retrieval models evaluation and interface issues web search including crawling link based algorithms and web metadata text web clustering classification text mining syllabus additional information staff contact information piazzza we strongly recommend students to post questions to the course page on www piazzza com instead of sending emails this forum enables students to discuss the questions they encounter in lectures or assignments here is a quick introduction video email if you have a question not appropriate for the piazzza forum eg one that is only relevant to your situation or one that reveals part of your solution to a homework question please email the staff mailing list at cs276 spr1011 staff lists stanford edu professor pandu nayak office none on campus office hours by appointment email nayak cs stanford edu professor prabhakar raghavan office none on campus office hours by appointment email pragh cs stanford edu ta sonali aggarwal office location b26 a ph 650 723 6319 office hours wednesday 4 30pm 6 30pm email sonali9 stanford edu ta ivan cui office location b26 b ph 650 736 1817 office hours monday 2 15pm 4 15pm email ivancui stanford edu ta valentin spitkovsky office location gates 228 second floor office hours thursday 2 15pm 4 15pm email vals stanford edu ta sandeep sripada office location b26 a ph 650 723 6319 office hours tuesday 5 30pm 7 30pm remember to carry your stanford id to open the basement doors email sss cs stanford edu announcements if you are not registered in the course but wish to receive course announcements you may subscribe to the guest mailing list cs276 spr1011 guests lists stanford edu textbook introduction to information retrieval by c manning p raghavan and h schutze cambridge university press available from the stanford bookstore or other fine retailers you can also download and print chapters at the book website prerequisites cs 103b and cs 107 and any one of cs 121 cs 145 or cs 161 or equivalent background programming experience will be necessary for the two practical exercises related courses introduction to computational advertising http www stanford edu class msande239\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"pa1-data/1/cs276.stanford.edu_\", 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试dev queries（提前构建好的查询与结果）是否正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results match for query: we are\n",
      "Results match for query: stanford class\n",
      "Results match for query: stanford students\n",
      "Results match for query: very cool\n",
      "Results match for query: the\n",
      "Results match for query: a\n",
      "Results match for query: the the\n",
      "Results match for query: stanford computer science\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 9):\n",
    "    with open('dev_queries/query.' + str(i)) as q:\n",
    "        query = q.read()\n",
    "        my_results = [os.path.normpath(path) for path in BSBI_instance.retrieve(query)]\n",
    "        with open('dev_output/' + str(i) + '.out') as o:\n",
    "            reference_results = [os.path.normpath(x.strip()) for x in o.readlines()]\n",
    "            assert my_results == reference_results, \"Results DO NOT match for query: \"+query.strip()\n",
    "        print(\"Results match for query:\", query.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果出现了错误，可以通过以下代码比较输出与正确结果的差异"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(my_results) - set(reference_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(reference_results) - set(my_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "确保你构建自己的查询来测试所有可能的边界情况，例如数据集中没有出现的terms，或者拖慢合并的频繁出现的terms。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 索引压缩  (30%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这部分，你将使用可变长字节编码对索引进行压缩。（gap-encoding可选，对gap进行编码）\n",
    "\n",
    "下面几个Python运算符可能对你有帮助"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 % 2 =  0\n",
      "10 % 3 =  1\n",
      "10 / 3 =  3.3333333333333335\n",
      "10 // 3 =  3\n"
     ]
    }
   ],
   "source": [
    "# Remainder (modulo) operator %\n",
    "\n",
    "print(\"10 % 2 = \", 10 % 2)\n",
    "print(\"10 % 3 = \", 10 % 3)\n",
    "\n",
    "# Integer division in Python 3 is done by using two slash signs\n",
    "\n",
    "print(\"10 / 3 = \", 10 / 3)\n",
    "print(\"10 // 3 = \", 10 // 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成下面的`CompressedPostings`类，我们将用它来替换`UncompressedPostings`。关于如何使用gap-encoding和可变长字节编码，你可以参考老师课件或者[Chapter 5](https://nlp.stanford.edu/IR-book/pdf/05comp.pdf)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: blue\">对于索引压缩这部分的工作，我选择使用VB和gap-encoding来实现。先讲解VB的原理：对于编码，假如有一个正整数n，先将其为二进制形式，再\n",
    "按每七位划分为一组（总位数不是7的倍数则从高位侧补0），再在非最后一组的其他每组前再填充一位0，而最后一组则填1，就实现了编码。解码就为逆过程。\n",
    "那么对于VB的实现，我们先涉及三个辅助方法，对于VBEncodeNumber，主要就是根据刚才的原理先将一个数n划分为7位一组，存储于列表中，由于2的7次方为128，\n",
    "所以对于填充1我们只需要将对应的数加128即可，其他填充0我们不需要作额外的处理。由于我们采取的是append的尾插法来存储数据，最后我们需要将列表中的\n",
    "元素位置进行反转。对于VBEncode，只是输入从一个数变成了一列表的数，只需要以此读取数据调用VBEncodeNumber再将得到的列表extend入初始化存储结果的列\n",
    "表，再遍历结束后返回该列表即可。而对于VBDecode，我们得到的输入即为调用VBEncode得到的列表，列表可能包含多个数的VB编码，因此我们需要判断一个数的\n",
    "相应VB编码终止于哪一个数，根据前面中延续位的知识我们可以发现，从上一个大于127的数之后的第一个数到下一个大于127的数直接的所有数都是一个数的VB编码，\n",
    "我们只需要将这些数根据高低位进行一定的处理即可得到该数的原十进制版本（具体方法在VBDecode已有完成，信息检索导论教材亦有伪代码，值得注意的是最后\n",
    "一个数要减去一个128，以去掉添加延续位对原数值可能造成的影响），并将其放入保存结果的列表。最后返回列表即可。\n",
    "<br>\n",
    "<br>\n",
    "接下来，我们正式开始Encode和Decode方法的实现，在此之前先介绍一下gap-encoding的概念。在我的理解中，gap-encoding还有另一个名字-差分，即除了列表\n",
    "的第一个元素外我们都保留的是原列表相应位置的项与其前一项的差值。对于Encode，我们也是先进行差分列表的求解，再使用前面实现的辅助函数对差分列表\n",
    "进行VB编码即可。最后将编码后的列表以字节串的形式返回即可。而对于Decode，过程正好相反即可，我们先将字节串转回编码后的列表，再使用VBDecode解码\n",
    "得到差分列表，借助差分的知识还原回原本的列表返回即可。\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompressedPostings:\n",
    "    #If you need any extra helper methods you can add them here \n",
    "    ### Begin your code\n",
    "    @staticmethod\n",
    "    def VBEncodeNumber(n):\n",
    "        byte = []\n",
    "        while n // 128 > 0:\n",
    "            temp_num = n % 128\n",
    "            byte.append(temp_num)\n",
    "            n = n // 128\n",
    "        byte.append(n)\n",
    "        byte[0] += 128\n",
    "        byte = byte[::-1]\n",
    "        return byte\n",
    "    \n",
    "    @staticmethod\n",
    "    def VBEncode(numbers):\n",
    "        bytestream = []\n",
    "        for num in numbers:\n",
    "            temp_list = CompressedPostings.VBEncodeNumber(num)\n",
    "            bytestream.extend(temp_list)\n",
    "        return bytestream\n",
    "    \n",
    "    @staticmethod\n",
    "    def VBDecode(bytestream):\n",
    "        numbers = []\n",
    "        n = 0\n",
    "        for byte in bytestream:\n",
    "            if byte >= 128:\n",
    "                n = n * 128 + (byte - 128)\n",
    "                numbers.append(n)\n",
    "                n = 0\n",
    "            else:\n",
    "                n = n * 128 + byte\n",
    "        return numbers\n",
    "    ### End your code\n",
    "    \n",
    "    @staticmethod\n",
    "    def encode(postings_list):\n",
    "        \"\"\"Encodes `postings_list` using gap encoding with variable byte \n",
    "        encoding for each gap\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        postings_list: List[int]\n",
    "            The postings list to be encoded\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        bytes: \n",
    "            Bytes reprsentation of the compressed postings list \n",
    "            (as produced by `array.tobytes` function)\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        result_list = []\n",
    "        for i in range(0, len(postings_list)):\n",
    "            result_list.append(0)\n",
    "        result_list[0] = postings_list[0]\n",
    "        for i in range(1, len(postings_list)):\n",
    "            result_list[i] = postings_list[i] - postings_list[i - 1]\n",
    "        res = CompressedPostings.VBEncode(result_list)\n",
    "        res_byte = array.array('B', res).tobytes()\n",
    "        return res_byte\n",
    "        ### End your code\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def decode(encoded_postings_list):\n",
    "        \"\"\"Decodes a byte representation of compressed postings list\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        encoded_postings_list: bytes\n",
    "            Bytes representation as produced by `CompressedPostings.encode` \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        List[int]\n",
    "            Decoded postings list (each posting is a docIds)\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        data = array.array('B')\n",
    "        data.frombytes(encoded_postings_list)\n",
    "        encoded_list = data.tolist()\n",
    "        postings_list = CompressedPostings.VBDecode(encoded_list)\n",
    "        result_list = []\n",
    "        for i in range(0, len(postings_list)):\n",
    "            result_list.append(0)\n",
    "        result_list[0] = postings_list[0]\n",
    "        for i in range(1, len(postings_list)):\n",
    "            result_list[i] = result_list[i - 1] + postings_list[i]\n",
    "        return result_list\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，如果你实现了额外的辅助函数，写一些测试样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin your code\n",
    "\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们实现了一个简单的函数来测试你编码的列表是否被正确解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_encode_decode(l):\n",
    "    e = CompressedPostings.encode(l)\n",
    "    d = CompressedPostings.decode(e)\n",
    "    assert d == l\n",
    "    print(l, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "写一些测试样例来确保文档列表的压缩和解压正常运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin your code\n",
    "\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在让我们创建一个新的输出文件夹并使用`CompressedPostings`来构建压缩索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    os.mkdir('output_dir_compressed')\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSBI_instance_compressed = BSBIIndex(data_dir='pa1-data', output_dir = 'output_dir_compressed', postings_encoding=CompressedPostings)\n",
    "BSBI_instance_compressed.index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1\\\\cs276.stanford.edu_',\n",
       " '1\\\\cs276a.stanford.edu_',\n",
       " '3\\\\infolab.stanford.edu_TR_CS-TN-95-21.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_an-appraisal-of-probabilistic-models-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_bayesian-network-approaches-to-ir-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_boolean-retrieval-2.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_computing-scores-in-a-complete-search-system-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_dictionaries-and-tolerant-retrieval-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_efficient-scoring-and-ranking-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_inexact-top-k-document-retrieval-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_probabilistic-information-retrieval-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_putting-it-all-together-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_references-and-further-reading-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_the-search-user-experience-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_vector-space-scoring-and-query-operator-interaction-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_weighted-zone-scoring-1.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_html_htmledition_wildcard-queries-2.html',\n",
       " '4\\\\nlp.stanford.edu_IR-book_newslides.html',\n",
       " '6\\\\www-nlp.stanford.edu_IR-book_essir2011_',\n",
       " '8\\\\www.stanford.edu_class_cs124_']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BSBI_instance_compressed = BSBIIndex(data_dir='pa1-data', output_dir = 'output_dir_compressed', postings_encoding=CompressedPostings)\n",
    "BSBI_instance_compressed.retrieve('boolean retrieval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "像之前一样，用已构造好的查询语句来测试是否正常运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results match for query: we are\n",
      "Results match for query: stanford class\n",
      "Results match for query: stanford students\n",
      "Results match for query: very cool\n",
      "Results match for query: the\n",
      "Results match for query: a\n",
      "Results match for query: the the\n",
      "Results match for query: stanford computer science\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 9):\n",
    "    with open('dev_queries/query.' + str(i)) as q:\n",
    "        query = q.read()\n",
    "        my_results = [os.path.normpath(path) for path in BSBI_instance_compressed.retrieve(query)]\n",
    "        with open('dev_output/' + str(i) + '.out') as o:\n",
    "            reference_results = [os.path.normpath(x.strip()) for x in o.readlines()]\n",
    "            assert my_results == reference_results, \"Results DO NOT match for query: \"+query.strip()\n",
    "        print(\"Results match for query:\", query.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 额外的编码方式 (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过补充`ECCompressedPostings`的`encode` 和 `decode`方法来实现一种额外的索引压缩方式。在我们课上学的就是**gamma-encoding** 。另外如果大家感兴趣的话也可以了解**Delta Encoding** ，[ALGORITHM SIMPLE-9](https://github.com/manning/CompressionAlgorithms#simple-9) 等。\n",
    "\n",
    "你应该以多字节（而不是bits）来存储倒排记录表，因为索引的长度和位置都存的是字节信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: blue\">对于额外编码方式这部分的工作，我选择使用gamma-encoding（具体实现方法还是参考信息检索教材）和gap-encoding。先介绍一下\n",
    "gamma-encoding的实现方法，对于一个正整数n，先将其转为二进制形式，计算此时的长度L，那么n的gamma编码方式即为L-1个1 + 0 + n的二进制形式去掉最高位\n",
    "（此处的 + 可以理解为字符串的拼接运算）。根据gamma-encoding的原理设计相应的辅助方法GammaEncode，具体的实现和刚才的原理相同，唯一值得注意的是Python\n",
    "中将十进制数转为二进制字符串会在最前面多加两位表示二进制的0b，因此在去掉最高位的基础上还要多去掉两位。\n",
    "<br>\n",
    "<br>\n",
    "而对于Encode函数，和索引压缩类似，我们使用相同的方法先得到输入列表的差分列表，再将差分列表的每一项使用辅助方法转化为相应编码后得到的字符串存储\n",
    "在相应存放结果的列表中。由于要求使用多字节而非比特来存储倒排记录表，因此先将res列表中每个gap所对应的字符串拼接在一起，接着对字符串\n",
    "进行处理，检查字符串的长度是否为8的倍数（因为1字节 = 8比特），如果不是则在最前面补0。将字符串切割为8位一组的子字符串，再将子字符串转为整型后都放入结果列表中，最后转为字节串的形式返回即可。\n",
    "<br>\n",
    "<br>\n",
    "对于Decode函数，我们先将字节串转为相应的列表，再将列表中的每一项都转为二进制字符串并进行拼接，接着对拼接后的字符串进行gamma-encoding的解码工作。\n",
    "对于解码，我们根据gamma-encoding的性质可以发现，一连串的1出现时，连续的1的位数总长即为相应编码数在字符串保留的位数，由于插入的1与原数的二进制形式\n",
    "（去掉最高位）间仅间隔一个0，因此我们借助一个指针和一个记录长度的变量就可以完成将字符串解码为差分列表。指针可以帮助我们记录连续的1的起点，也可以\n",
    "帮助我们定位截取字符串的开始，遇到连续的1时先获取连续的1的位数总长，再将指针当前值加上位数总长再加一（跳过0）就是相应数对于字符串的开始，截取字符串\n",
    "再于最高位拼接上1最后转回十进制数就得到了相应数，当然结束后要将指针跨越相应数的二进制部分指向下一个数的部分，并将长度重新置为0。当然，值得注意的是\n",
    "，我们在编码中为了实现多字节存储可能在字符串前插入了用来补位的多余的0，因此在处理前我们需要先将前导0全部去掉，不过注意特殊情况，如果前导0的数量为8，那么第一个数一定为1（因为不会直接补8个0），需要对其进行直接放入列表的操作。将得到的所有数\n",
    "都存入列表就得到了差分列表，再利用差分知识计算就可以得到原始列表，将原始列表返回即可。\n",
    "<br>\n",
    "<br>\n",
    "但是既然已经思考到这里，你肯定已经很容易想到了一个有意思的问题：如果列表第一个数是1，后面又有多个gap为1，那岂不是束手无策了？很简单，我们在编码过程中直接从2开始不就好了。对Encode和Decode函数进行修改，给每个编码前的gap都加1，在解码后得到的gap再减1，就得到了原gap了，这样就避免了这个问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECCompressedPostings:\n",
    "    #If you need any extra helper methods you can add them here \n",
    "    ### Begin your code\n",
    "    @staticmethod\n",
    "    def GammaEncode(num):\n",
    "        n = bin(num)[2:]\n",
    "        return (len(n) - 1) * '1' + '0' + n[1:]\n",
    "    \n",
    "    ### End your code\n",
    "\n",
    "    @staticmethod\n",
    "    def encode(postings_list):\n",
    "        \"\"\"Encodes `postings_list` \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        postings_list: List[int]\n",
    "            The postings list to be encoded\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        bytes: \n",
    "            Bytes reprsentation of the compressed postings list \n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        result_list = []\n",
    "        for i in range(0, len(postings_list)):\n",
    "            result_list.append(0)\n",
    "        result_list[0] = postings_list[0] + 1\n",
    "        for i in range(1, len(postings_list)):\n",
    "            result_list[i] = postings_list[i] - postings_list[i - 1] + 1\n",
    "        res = []\n",
    "        for i in range(0, len(result_list)):\n",
    "            res.append(ECCompressedPostings.GammaEncode(result_list[i]))\n",
    "        string = None\n",
    "        for i in res:\n",
    "            if string is None:\n",
    "                string = i\n",
    "            else:\n",
    "                string = string + i\n",
    "        string_len = len(string) % 8\n",
    "        need_num = (8 - string_len) % 8\n",
    "        string = need_num * '0' + string\n",
    "        byte_stream = bytearray()\n",
    "        for i in range(0, len(string), 8):\n",
    "            byte_chunk = int(string[i: i + 8], 2)\n",
    "            byte_stream.append(byte_chunk)\n",
    "        return array.array('B',byte_stream).tobytes()\n",
    "        ### End your code\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def decode(encoded_postings_list):\n",
    "        \"\"\"Decodes a byte representation of compressed postings list\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        encoded_postings_list: bytes\n",
    "            Bytes representation as produced by `CompressedPostings.encode` \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        List[int]\n",
    "            Decoded postings list (each posting is a docId)\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "        postings_list = array.array('B')\n",
    "        postings_list.frombytes(encoded_postings_list)\n",
    "        postings_str = ''.join([bin(byte)[2:].zfill(8) for byte in postings_list])\n",
    "        result = []\n",
    "        ptr = 0\n",
    "        base_str = 0\n",
    "        while postings_str[base_str] == '0':\n",
    "            base_str += 1\n",
    "        #if base_str == 8:\n",
    "        #    result.append(1)    //修改后的代码就没有1了，因此这里也不需要再对可能的1进行额外的判断了\n",
    "        postings_str = postings_str[base_str:]\n",
    "        while ptr < len(postings_str):\n",
    "            bin_length = 0\n",
    "            while ptr < len(postings_str) and postings_str[ptr] == '1':\n",
    "                ptr += 1\n",
    "                bin_length += 1\n",
    "            ptr += 1\n",
    "            if bin_length > 0:\n",
    "                temp_str = postings_str[ptr: ptr + bin_length]\n",
    "                res = int('1' + temp_str, 2)\n",
    "                result.append(res)\n",
    "                ptr += bin_length\n",
    "            else:\n",
    "                result.append(1)\n",
    "        res_list = [result[0] - 1]\n",
    "        for i in range(1, len(result)):\n",
    "            res_list.append(res_list[i - 1] + result[i] - 1)\n",
    "        return res_list\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: blue\">让我们编写一段简单的测试代码来验证一下Gamma编码的正确性吧</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 10, 45, 169]\n",
      "b'\\t$\\x9a\\xf8\\x9f\\xbd'\n",
      "[1, 2, 3, 4, 5, 10, 45, 169]\n"
     ]
    }
   ],
   "source": [
    "postings = [1,2,3,4,5,10,45,169]\n",
    "#postings = [1,3,6,10,19,32,56,567,1592]\n",
    "encoded = ECCompressedPostings.encode(postings)\n",
    "decoded = ECCompressedPostings.decode(encoded)\n",
    "\n",
    "print(postings)\n",
    "print(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: blue\">通过本次实验，我对课上所讲的BSBI、布尔检索以及可变长编码和Gamma编码（部分自学）在理论的基础上进行了实际的实现，\n",
    "并加深了我对相应知识的理解与掌握。本次实验的难点主要在于从庞大的框架中知道需要用什么、该怎么用、怎么用更好，需要结合课上所讲述的知识，\n",
    "并在作业中进行实际尝试，才能解决相应的问题。</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作业提交\n",
    "\n",
    "本次作业用时y约3周，截止日期为9.30（国庆节前）。请大家在截止日期前将代码（包含运行结果，测试内容不作要求），实验报告（可单独撰写，也可整合在jupyter notebook中）一起提交到ir24fall@163.com，邮件和文件命名方式均为`学号_姓名_hw1`，如`1811412_戚晓睿_hw1.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
